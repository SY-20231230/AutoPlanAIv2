import os
import json
import time
import itertools
from github import Github, RateLimitExceededException, UnknownObjectException
from dotenv import load_dotenv

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

def get_github_instance():
    token = os.getenv("GITHUB_TOKEN")
    if not token:
        print("âš ï¸ ê²½ê³ : GitHub í† í°ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. API ìš”ì²­ ì œí•œì´ ë§¤ìš° ë‚®ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        print("     .env íŒŒì¼ì— 'GITHUB_TOKEN=your_personal_access_token'ì„ ì¶”ê°€í•´ì£¼ì„¸ìš”.")
        return Github()
    return Github(token)

def search_repositories(g, keywords, max_repos_per_query=5):
    all_repos = {}

    # 1) ë‹¨ì¼ í‚¤ì›Œë“œ ê²€ìƒ‰
    for keyword in keywords:
        query = f'"{keyword}" in:readme,description'
        try:
            repositories = g.search_repositories(query, sort="stars", order="desc")
            print(f"ğŸ” [ë‹¨ì¼] ì¿¼ë¦¬: '{query}'")
            print(f"  - '{keyword}' í‚¤ì›Œë“œë¡œ {repositories.totalCount}ê°œì˜ ë¦¬í¬ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤. ìƒìœ„ {max_repos_per_query}ê°œë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.")
            count = 0
            for repo in repositories:
                if repo.id not in all_repos:
                    all_repos[repo.id] = repo
                    count += 1
                    if count >= max_repos_per_query:
                        break
            time.sleep(2)
        except RateLimitExceededException:
            print("âŒ Rate limit ì´ˆê³¼. ëŒ€ê¸°...")
            break
        except Exception as e:
            print(f"âŒ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            continue

    # 2) 2ê°œ í‚¤ì›Œë“œì”© ì¡°í•© ê²€ìƒ‰
    for combo in itertools.combinations(keywords, 2):
        combo_query = " ".join([f'"{kw}"' for kw in combo])
        query = f"{combo_query} in:readme,description"
        try:
            repositories = g.search_repositories(query, sort="stars", order="desc")
            print(f"ğŸ” [ë³µìˆ˜] ì¿¼ë¦¬: '{query}'")
            print(f"  - {combo} ì¡°í•©ìœ¼ë¡œ {repositories.totalCount}ê°œì˜ ë¦¬í¬ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤. ìƒìœ„ {max_repos_per_query}ê°œë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.")
            count = 0
            for repo in repositories:
                if repo.id not in all_repos:
                    all_repos[repo.id] = repo
                    count += 1
                    if count >= max_repos_per_query:
                        break
            time.sleep(2)
        except RateLimitExceededException:
            print("âŒ Rate limit ì´ˆê³¼. ëŒ€ê¸°...")
            break
        except Exception as e:
            print(f"âŒ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            continue

    print(f"\nğŸ“Š ì´ {len(all_repos)}ê°œì˜ ê³ ìœ í•œ ë¦¬í¬ì§€í† ë¦¬ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
    return list(all_repos.values())


def get_readme_content(repo):
    try:
        readme = repo.get_readme()
        content = readme.decoded_content.decode("utf-8")
        return content
    except UnknownObjectException:
        return None
    except Exception as e:
        print(f"      - â—ï¸ READMEë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({repo.full_name}): {e}")
        return None

def keyword_match_count(text, keywords):
    text = (text or "").lower()
    return sum(1 for kw in keywords if kw.lower() in text)

def matched_keywords_list(text, keywords):
    text = (text or "").lower()
    return [kw for kw in keywords if kw.lower() in text]

def crawl_github():
    try:
        with open("keywords.json", "r", encoding="utf-8") as f:
            keywords = json.load(f)
    except FileNotFoundError:
        print("âŒ 'keywords.json' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € keyword_extractor.pyë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        return

    print("ğŸ”‘ í‚¤ì›Œë“œ ë¡œë“œ ì™„ë£Œ:", keywords)
    g = get_github_instance()
    repos = search_repositories(g, keywords)
    if not repos:
        print("ğŸš« ì²˜ë¦¬í•  ë¦¬í¬ì§€í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    crawled_data = []
    print("\nğŸ“‚ ê° ë¦¬í¬ì§€í† ë¦¬ì˜ README.md íŒŒì¼ ìˆ˜ì§‘ ë° í‚¤ì›Œë“œ ë§¤ì¹­ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

    for i, repo in enumerate(repos):
        print(f"  - ({i+1}/{len(repos)}) ì²˜ë¦¬ ì¤‘: {repo.full_name} (â­{repo.stargazers_count})")
        time.sleep(1)
        readme_content = get_readme_content(repo)
        combined = (repo.description or "") + "\n" + (readme_content or "")
        match_count = keyword_match_count(combined, keywords)
        matched_list = matched_keywords_list(combined, keywords)
        if match_count >= 2:  # *** 2ê°œ ì´ìƒ ê²¹ì¹˜ë©´ ì €ì¥! ***
            crawled_data.append({
                "name": repo.full_name,
                "url": repo.html_url,
                "stars": repo.stargazers_count,
                "description": repo.description,
                "readme": readme_content,
                "matched_keywords": matched_list,
                "matched_count": match_count
            })
            print(f"      - âœ… {match_count}ê°œ í‚¤ì›Œë“œ ê²¹ì¹¨ â†’ ì €ì¥")
        else:
            print(f"      - âŒ í‚¤ì›Œë“œ {match_count}ê°œ ê²¹ì¹¨(2ê°œ ë¯¸ë§Œ) â†’ ì €ì¥ ì•ˆí•¨")

    output_filename = "github_repositories.json"
    with open(output_filename, "w", encoding="utf-8") as f:
        json.dump(crawled_data, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… (í‚¤ì›Œë“œ 2ê°œ ì´ìƒ ê²¹ì¹œ) {len(crawled_data)}ê°œ ë¦¬í¬ë§Œ '{output_filename}'ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    crawl_github()
