import os
import json
import time
import itertools
import base64
import chardet
from github import Github, RateLimitExceededException, UnknownObjectException, GithubException
from dotenv import load_dotenv

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

def get_github_instance():
    token = os.getenv("GITHUB_TOKEN")
    if not token:
        print("âš ï¸ ê²½ê³ : GitHub í† í°ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. API ìš”ì²­ ì œí•œì´ ë§¤ìš° ë‚®ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        print("     .env íŒŒì¼ì— 'GITHUB_TOKEN=your_personal_access_token'ì„ ì¶”ê°€í•´ì£¼ì„¸ìš”.")
        return Github()
    return Github(token)


def search_repositories(g, keywords, max_repos_per_query=5):
    all_repos = {}

    # 1) ë‹¨ì¼ í‚¤ì›Œë“œ ê²€ìƒ‰
    for keyword in keywords:
        query = f'"{keyword}" in:readme,description'
        try:
            repositories = g.search_repositories(query, sort="stars", order="desc")
            print(f"ğŸ” [ë‹¨ì¼] ì¿¼ë¦¬: '{query}'")
            print(f"  - '{keyword}' í‚¤ì›Œë“œë¡œ {repositories.totalCount}ê°œì˜ ë¦¬í¬ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤. ìƒìœ„ {max_repos_per_query}ê°œë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.")
            count = 0
            for repo in repositories:
                if repo.id not in all_repos:
                    all_repos[repo.id] = repo
                    count += 1
                    if count >= max_repos_per_query:
                        break
            time.sleep(2)
        except RateLimitExceededException:
            print("âŒ Rate limit ì´ˆê³¼. ëŒ€ê¸°...")
            break
        except Exception as e:
            print(f"âŒ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            continue

    # 2) 2ê°œ í‚¤ì›Œë“œì”© ì¡°í•© ê²€ìƒ‰
    for combo in itertools.combinations(keywords, 2):
        combo_query = " ".join([f'"{kw}"' for kw in combo])
        query = f"{combo_query} in:readme,description"
        try:
            repositories = g.search_repositories(query, sort="stars", order="desc")
            print(f"ğŸ” [ë³µìˆ˜] ì¿¼ë¦¬: '{query}'")
            print(f"  - {combo} ì¡°í•©ìœ¼ë¡œ {repositories.totalCount}ê°œì˜ ë¦¬í¬ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤. ìƒìœ„ {max_repos_per_query}ê°œë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.")
            count = 0
            for repo in repositories:
                if repo.id not in all_repos:
                    all_repos[repo.id] = repo
                    count += 1
                    if count >= max_repos_per_query:
                        break
            time.sleep(2)
        except RateLimitExceededException:
            print("âŒ Rate limit ì´ˆê³¼. ëŒ€ê¸°...")
            break
        except Exception as e:
            print(f"âŒ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            continue

    print(f"\nğŸ“Š ì´ {len(all_repos)}ê°œì˜ ê³ ìœ í•œ ë¦¬í¬ì§€í† ë¦¬ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
    return list(all_repos.values())


def get_readme_content(repo):
    """
    READMEë¥¼ ì•ˆì „í•˜ê²Œ ë¬¸ìì—´ë¡œ ë°˜í™˜.
    - PyGithub ContentFile.decoded_contentê°€ ìˆìœ¼ë©´ bytes â†’ UTF-8 ì‹œë„, ì‹¤íŒ¨ ì‹œ chardetë¡œ ì¶”ì •
    - content/encoding ê²½ë¡œëŠ” base64 ìš°ì„  ì²˜ë¦¬
    - ì–´ë–¤ í˜•ì‹ì´ë“  ê²°êµ­ strì„ ë°˜í™˜(ì—ëŸ¬ ì‹œ ë¹ˆ ë¬¸ìì—´)
    """
    try:
        readme = repo.get_readme()
    except (UnknownObjectException, GithubException):
        return ""  # README ì—†ìŒ/ì ‘ê·¼ ë¶ˆê°€

    # 1) ê°€ì¥ ì•ˆì „: decoded_content (bytes)
    data = getattr(readme, "decoded_content", None)
    if isinstance(data, (bytes, bytearray)):
        try:
            return data.decode("utf-8", errors="replace")
        except Exception:
            enc = (chardet.detect(data) or {}).get("encoding") or "utf-8"
            try:
                return data.decode(enc, errors="replace")
            except Exception:
                return ""

    # 2) content + encoding (ì¼ë¶€ëŠ” encoding=Noneì´ê±°ë‚˜ 'base64'ê°€ ì•„ë‹˜)
    content = getattr(readme, "content", None)
    encname = getattr(readme, "encoding", None)

    # contentê°€ strì´ë©´ bytesë¡œ ë³€í™˜
    if isinstance(content, str):
        content = content.encode("utf-8", errors="ignore")

    if encname == "base64" and content:
        try:
            raw = base64.b64decode(content)
            try:
                return raw.decode("utf-8")
            except UnicodeDecodeError:
                enc = (chardet.detect(raw) or {}).get("encoding") or "utf-8"
                return raw.decode(enc, errors="replace")
        except Exception:
            # base64 ë””ì½”ë”© ì‹¤íŒ¨ ì‹œ í´ë°±
            pass

    # 3) ë§ˆì§€ë§‰ í´ë°±: contentë¥¼ ê·¸ëƒ¥ ì¶”ì • ë””ì½”ë”©
    if isinstance(content, (bytes, bytearray)):
        enc = (chardet.detect(content) or {}).get("encoding") or "utf-8"
        try:
            return content.decode(enc, errors="replace")
        except Exception:
            return ""

    return ""  # ì–´ë–¤ ê²½ë¡œë¡œë„ ë³µêµ¬ ëª»í•˜ë©´ ë¹ˆ ë¬¸ìì—´


def keyword_match_count(text, keywords):
    text = (text or "").lower()
    return sum(1 for kw in keywords if kw.lower() in text)


def matched_keywords_list(text, keywords):
    text = (text or "").lower()
    return [kw for kw in keywords if kw.lower() in text]


def crawl_github():
    try:
        with open("keywords.json", "r", encoding="utf-8") as f:
            keywords = json.load(f)
    except FileNotFoundError:
        print("âŒ 'keywords.json' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € keyword_extractor.pyë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        return

    print("ğŸ”‘ í‚¤ì›Œë“œ ë¡œë“œ ì™„ë£Œ:", keywords)
    g = get_github_instance()
    repos = search_repositories(g, keywords)
    if not repos:
        print("ğŸš« ì²˜ë¦¬í•  ë¦¬í¬ì§€í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    crawled_data = []
    print("\nğŸ“‚ ê° ë¦¬í¬ì§€í† ë¦¬ì˜ README.md íŒŒì¼ ìˆ˜ì§‘ ë° í‚¤ì›Œë“œ ë§¤ì¹­ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

    for i, repo in enumerate(repos):
        print(f"  - ({i+1}/{len(repos)}) ì²˜ë¦¬ ì¤‘: {repo.full_name} (â­{getattr(repo, 'stargazers_count', 0)})")
        time.sleep(1)

        readme_content = get_readme_content(repo)  # âš™ï¸ í•­ìƒ str ë°˜í™˜ (ì‹¤íŒ¨ ì‹œ "")
        combined = f"{repo.description or ''}\n{readme_content or ''}"

        match_count = keyword_match_count(combined, keywords)
        matched_list = matched_keywords_list(combined, keywords)

        if match_count >= 2:  # *** 2ê°œ ì´ìƒ ê²¹ì¹˜ë©´ ì €ì¥! ***
            crawled_data.append({
                "name": repo.full_name,
                "url": repo.html_url,
                "stars": int(getattr(repo, "stargazers_count", 0) or 0),
                "description": repo.description,
                "readme": readme_content,
                "matched_keywords": matched_list,
                "matched_count": match_count
            })
            print(f"      - âœ… {match_count}ê°œ í‚¤ì›Œë“œ ê²¹ì¹¨ â†’ ì €ì¥")
        else:
            print(f"      - âŒ í‚¤ì›Œë“œ {match_count}ê°œ ê²¹ì¹¨(2ê°œ ë¯¸ë§Œ) â†’ ì €ì¥ ì•ˆí•¨")

    output_filename = "github_repositories.json"
    with open(output_filename, "w", encoding="utf-8") as f:
        json.dump(crawled_data, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… (í‚¤ì›Œë“œ 2ê°œ ì´ìƒ ê²¹ì¹œ) {len(crawled_data)}ê°œ ë¦¬í¬ë§Œ '{output_filename}'ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    crawl_github()
